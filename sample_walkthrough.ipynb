{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Arbitrage with Correlation Matrix Clustering\n",
    "# Enhanced Example Notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the implementation of the paper \"Correlation Matrix Clustering for Statistical Arbitrage Portfolios\" by Cartea, Cucuringu, and Jin (2023), with significant enhancements to the original methodology.\n",
    "\n",
    "The approach consists of two main steps:\n",
    "1. **Clustering**: Group stocks based on their residual return correlation matrix using graph clustering algorithms\n",
    "2. **Portfolio Construction**: Build mean-reverting portfolios within each cluster\n",
    "\n",
    "We'll go through the entire process step by step, from data preparation to portfolio evaluation, including all the enhanced features of our implementation."
   ],
   "id": "340ff4e20da10b6d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ],
   "id": "1db342a408818e48"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_palette('tab10')\n",
    "\n",
    "# Import our custom modules\n",
    "from data_preprocessing import prepare_stock_data, fetch_stock_data, calculate_returns, prepare_multifactor_data\n",
    "from correlation_matrix import compute_correlation_matrix, decompose_correlation_matrix\n",
    "from clustering import (\n",
    "    cluster_stocks, get_clusters_dict, determine_clusters_mp, determine_clusters_variance,\n",
    "    hierarchical_clustering, density_based_clustering, deep_embedding_clustering\n",
    ")\n",
    "from portfolio_construction import (\n",
    "    identify_winners_losers, construct_arbitrage_portfolios, calculate_portfolio_returns\n",
    ")\n",
    "from utils import (\n",
    "    calculate_annualized_return,\n",
    "    calculate_sharpe_ratio,\n",
    "    calculate_sortino_ratio,\n",
    "    plot_correlation_matrix,\n",
    "    plot_clusters,\n",
    "    get_eigenvalue_distribution\n",
    ")\n",
    "from visualization_plotly import (\n",
    "    plot_cumulative_returns,\n",
    "    plot_performance_metrics,\n",
    "    plot_drawdowns,\n",
    "    plot_monthly_returns_heatmap,\n",
    "    plot_transaction_costs,\n",
    "    create_performance_report\n",
    ")\n",
    "from backtest import (\n",
    "    StatisticalArbitrageBacktest, run_industry_benchmark,\n",
    "    time_series_cross_validation, monte_carlo_backtest, walk_forward_optimization\n",
    ")\n",
    "from fama_french import get_fama_french_industries, get_industry_names\n",
    "from adaptive_parameters import AdaptiveParameterManager\n",
    "from alternative_data import (\n",
    "    SentimentDataProcessor, OptionsDataProcessor, OrderFlowProcessor\n",
    ")\n",
    "from portfolio_integration import PortfolioAllocator, calculate_information_ratio_impact"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f3d361b187681d33"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We'll start by defining a universe of stocks and fetching historical price data. For this example, we'll use a diverse set of large-cap stocks from different sectors."
   ],
   "id": "48ae1d4f2347468d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define parameters\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-12-31'\n",
    "beta_window = 60  # Days for beta calculation\n",
    "\n",
    "# Define a universe of stocks from different sectors\n",
    "tickers = [\n",
    "    # Technology\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'META', 'NVDA', 'INTC', 'AMD', 'ADBE', 'CRM', 'CSCO',\n",
    "    # Financial\n",
    "    'JPM', 'BAC', 'GS', 'MS', 'V', 'MA', 'PYPL', \n",
    "    # Healthcare\n",
    "    'JNJ', 'PFE', 'MRK', 'ABT', 'UNH',\n",
    "    # Consumer\n",
    "    'AMZN', 'WMT', 'PG', 'KO', 'MCD', 'NKE', 'SBUX',\n",
    "    # Industrial\n",
    "    'GE', 'BA', 'CAT', 'MMM', 'HON',\n",
    "    # Energy\n",
    "    'XOM', 'CVX', 'COP',\n",
    "    # Telecom\n",
    "    'T', 'VZ', 'TMUS',\n",
    "    # Utilities\n",
    "    'NEE', 'DUK', 'SO'\n",
    "]\n",
    "\n",
    "print(f\"Using {len(tickers)} stocks for the analysis\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d36d0d394825ef99"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Traditional CAPM Approach\n",
    "\n",
    "First, let's use the traditional CAPM approach to calculate residual returns by removing the market factor."
   ],
   "id": "1470e8adc0208fc2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fetch data and calculate returns, betas, and residual returns\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    returns, betas, residual_returns = prepare_stock_data(\n",
    "        tickers, \n",
    "        start_date, \n",
    "        end_date, \n",
    "        beta_window=beta_window\n",
    "    )\n",
    "    print(f\"Data preparation completed in {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Check the dimensions of our data\n",
    "    print(f\"Returns shape: {returns.shape}\")\n",
    "    print(f\"Betas shape: {betas.shape}\")\n",
    "    print(f\"Residual returns shape: {residual_returns.shape}\")\n",
    "    \n",
    "    # Check which tickers we actually have data for\n",
    "    actual_tickers = returns.columns.drop('SPY').tolist()\n",
    "    print(f\"Actual number of stocks with data: {len(actual_tickers)}\")\n",
    "    \n",
    "    missing_tickers = [ticker for ticker in tickers if ticker not in actual_tickers]\n",
    "    if missing_tickers:\n",
    "        print(f\"Missing data for: {missing_tickers}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "    \n",
    "    # For demonstration purposes, create mock data if fetching fails\n",
    "    print(\"Creating mock data for demonstration...\")\n",
    "    \n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "    mock_returns = pd.DataFrame(\n",
    "        np.random.normal(0.0005, 0.015, (len(dates), len(tickers) + 1)),\n",
    "        index=dates,\n",
    "        columns=tickers + ['SPY']\n",
    "    )\n",
    "    \n",
    "    mock_betas = pd.DataFrame(\n",
    "        np.random.normal(1.0, 0.3, (len(dates) - beta_window, len(tickers))),\n",
    "        index=dates[beta_window:],\n",
    "        columns=tickers\n",
    "    )\n",
    "    \n",
    "    mock_residual_returns = pd.DataFrame(\n",
    "        np.random.normal(0.0, 0.01, (len(dates) - beta_window, len(tickers))),\n",
    "        index=dates[beta_window:],\n",
    "        columns=tickers\n",
    "    )\n",
    "    \n",
    "    returns = mock_returns\n",
    "    betas = mock_betas\n",
    "    residual_returns = mock_residual_returns\n",
    "    actual_tickers = tickers"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "25a6beaafe96c44e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Enhanced Multi-Factor Models\n",
    "\n",
    "Now, let's use the enhanced multi-factor models approach to better isolate stock-specific returns."
   ],
   "id": "4dbc19ae94d039f1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use Fama-French 5-factor model\n",
    "start_time = time.time()\n",
    "ff_returns, ff_factors, ff_residuals, ff_data = prepare_multifactor_data(\n",
    "    actual_tickers, \n",
    "    start_date, \n",
    "    end_date, \n",
    "    factor_model='fama_french'\n",
    ")\n",
    "print(f\"Fama-French model preparation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Use PCA-based factor model\n",
    "start_time = time.time()\n",
    "pca_returns, pca_factors, pca_residuals, pca_data = prepare_multifactor_data(\n",
    "    actual_tickers, \n",
    "    start_date, \n",
    "    end_date, \n",
    "    factor_model='pca', \n",
    "    n_components=5\n",
    ")\n",
    "print(f\"PCA model preparation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Compare the results - check correlation between CAPM and enhanced residuals\n",
    "capm_pca_corr = pd.DataFrame(\n",
    "    index=actual_tickers,\n",
    "    columns=['CAPM_FF_Corr', 'CAPM_PCA_Corr', 'FF_PCA_Corr']\n",
    ")\n",
    "\n",
    "for ticker in actual_tickers:\n",
    "    if ticker in ff_residuals.columns and ticker in pca_residuals.columns:\n",
    "        capm_pca_corr.loc[ticker, 'CAPM_FF_Corr'] = residual_returns[ticker].corr(ff_residuals[ticker])\n",
    "        capm_pca_corr.loc[ticker, 'CAPM_PCA_Corr'] = residual_returns[ticker].corr(pca_residuals[ticker])\n",
    "        capm_pca_corr.loc[ticker, 'FF_PCA_Corr'] = ff_residuals[ticker].corr(pca_residuals[ticker])\n",
    "\n",
    "print(\"\\nCorrelation between different residualization methods:\")\n",
    "print(capm_pca_corr.describe())\n",
    "\n",
    "# Visualize the distribution of correlations\n",
    "fig = px.box(\n",
    "    capm_pca_corr.melt(), \n",
    "    y=\"value\", \n",
    "    x=\"variable\", \n",
    "    title=\"Correlation Between Different Residualization Methods\",\n",
    "    labels={\"value\": \"Correlation\", \"variable\": \"Methods Compared\"}\n",
    ")\n",
    "fig.update_layout(yaxis_range=[-1, 1])\n",
    "fig.show()\n",
    "\n",
    "# For the rest of this notebook, we'll continue with the CAPM residuals for simplicity\n",
    "# But in practice, you might want to use the enhanced methods for better results"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "c6a47e4f0ee4b7d4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Let's examine the distribution of betas and residual returns:\n",
    "\n",
    "# Plot the distribution of betas for the most recent period\n",
    "recent_betas = betas.iloc[-1]\n",
    "\n",
    "fig = px.bar(\n",
    "    x=recent_betas.index, \n",
    "    y=recent_betas.values,\n",
    "    title='Current Beta Values by Stock',\n",
    "    labels={'x': 'Stock', 'y': 'Beta Value'}\n",
    ")\n",
    "fig.add_hline(y=1.0, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.update_layout(xaxis_tickangle=-90)\n",
    "fig.show()\n",
    "\n",
    "# Plot the distribution of residual returns vs raw returns\n",
    "# Get statistics for the most recent period (last 60 days)\n",
    "recent_raw_returns = returns.iloc[-60:].drop('SPY', axis=1)\n",
    "recent_residual_returns = residual_returns.iloc[-60:]\n",
    "\n",
    "# Compute volatility (standard deviation)\n",
    "raw_volatility = recent_raw_returns.std()\n",
    "residual_volatility = recent_residual_returns.std()\n",
    "\n",
    "# Create scatter plot\n",
    "fig = px.scatter(\n",
    "    x=raw_volatility, \n",
    "    y=residual_volatility,\n",
    "    text=raw_volatility.index,\n",
    "    title='Raw Return vs Residual Return Volatility',\n",
    "    labels={'x': 'Raw Return Volatility', 'y': 'Residual Return Volatility'}\n",
    ")\n",
    "fig.update_traces(textposition='top center')\n",
    "fig.show()\n",
    "\n",
    "# Volatility reduction after removing market factor\n",
    "volatility_reduction = (1 - residual_volatility / raw_volatility) * 100\n",
    "avg_reduction = volatility_reduction.mean()\n",
    "\n",
    "print(f\"Average volatility reduction after removing market factor: {avg_reduction:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "e313574bf5bd512e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Matrix Analysis\n",
    "\n",
    "Next, we'll compute and analyze the correlation matrix of residual returns."
   ],
   "id": "e532364ef718c316"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compute correlation matrix using the last 60 days of residual returns\n",
    "correlation_window = 60\n",
    "correlation_matrix = compute_correlation_matrix(residual_returns.iloc[-correlation_window:])\n",
    "\n",
    "# Visualize the correlation matrix using Plotly for interactivity\n",
    "fig = px.imshow(\n",
    "    correlation_matrix,\n",
    "    title='Residual Returns Correlation Matrix',\n",
    "    labels=dict(x=\"Stock\", y=\"Stock\", color=\"Correlation\"),\n",
    "    x=correlation_matrix.columns,\n",
    "    y=correlation_matrix.columns,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    zmin=-1, zmax=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Decompose the correlation matrix into positive and negative components\n",
    "positive_matrix, negative_matrix = decompose_correlation_matrix(correlation_matrix)\n",
    "\n",
    "# Visualize the positive matrix\n",
    "fig = px.imshow(\n",
    "    positive_matrix,\n",
    "    title='Positive Correlations',\n",
    "    labels=dict(x=\"Stock\", y=\"Stock\", color=\"Correlation\"),\n",
    "    x=positive_matrix.columns,\n",
    "    y=positive_matrix.columns,\n",
    "    color_continuous_scale='Blues',\n",
    "    zmin=0, zmax=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Visualize the negative matrix (absolute values)\n",
    "fig = px.imshow(\n",
    "    negative_matrix,\n",
    "    title='Negative Correlations (Absolute Values)',\n",
    "    labels=dict(x=\"Stock\", y=\"Stock\", color=\"Correlation\"),\n",
    "    x=negative_matrix.columns,\n",
    "    y=negative_matrix.columns,\n",
    "    color_continuous_scale='Reds',\n",
    "    zmin=0, zmax=1\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "536522e509423afa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the eigenvalue distribution to determine the optimal number of clusters:"
   ],
   "id": "b31f498b8a848620"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate the number of clusters using different methods\n",
    "n_clusters_mp = determine_clusters_mp(correlation_matrix, correlation_window)\n",
    "n_clusters_var = determine_clusters_variance(correlation_matrix, variance_explained=0.9)\n",
    "\n",
    "print(f\"Number of clusters (Marchenko-Pastur): {n_clusters_mp}\")\n",
    "print(f\"Number of clusters (90% Variance): {n_clusters_var}\")\n",
    "\n",
    "# Analyze eigenvalue distribution\n",
    "eigenvalues, variance_ratios = get_eigenvalue_distribution(correlation_matrix)\n",
    "\n",
    "# Eigenvalue distribution plot using Plotly\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Eigenvalue Distribution', 'Cumulative Variance Explained')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(eigenvalues) + 1)),\n",
    "        y=eigenvalues,\n",
    "        mode='lines+markers',\n",
    "        name='Eigenvalues'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(1, len(variance_ratios) + 1)),\n",
    "        y=variance_ratios,\n",
    "        mode='lines+markers',\n",
    "        name='Cumulative Variance'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Add line for 90% variance explained\n",
    "fig.add_shape(\n",
    "    type='line',\n",
    "    x0=0,\n",
    "    y0=0.9,\n",
    "    x1=len(variance_ratios),\n",
    "    y1=0.9,\n",
    "    line=dict(color='red', dash='dash'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    width=900,\n",
    "    title_text='Eigenvalue Analysis for Cluster Determination'\n",
    ")\n",
    "fig.update_xaxes(title_text='Eigenvalue Number', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Number of Eigenvalues', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Eigenvalue', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Cumulative Variance Ratio', row=1, col=2)\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "fcfa9172f6eafaa4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering"
   ],
   "id": "61af7f12f34894a8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Clustering Methods\n",
    "\n",
    "First, let's apply the basic clustering methods from the original paper:"
   ],
   "id": "144024b27767530e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply different clustering methods\n",
    "clustering_methods = ['spectral', 'signed_laplacian_sym', 'signed_laplacian_rw', 'sponge', 'sponge_sym']\n",
    "clustering_results = {}\n",
    "\n",
    "# Use the MP method for determining number of clusters\n",
    "n_clusters = n_clusters_mp\n",
    "\n",
    "for method in clustering_methods:\n",
    "    # Cluster the stocks\n",
    "    labels, actual_n_clusters = cluster_stocks(\n",
    "        correlation_matrix, \n",
    "        method=method, \n",
    "        n_clusters=n_clusters\n",
    "    )\n",
    "    \n",
    "    # Get clusters as dictionary\n",
    "    clusters = get_clusters_dict(labels, correlation_matrix.columns.tolist())\n",
    "    \n",
    "    # Store results\n",
    "    clustering_results[method] = {\n",
    "        'labels': labels,\n",
    "        'n_clusters': actual_n_clusters,\n",
    "        'clusters': clusters\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{method.upper()} clustering with {actual_n_clusters} clusters:\")\n",
    "    for cluster_id, cluster_tickers in clusters.items():\n",
    "        print(f\"Cluster {cluster_id} ({len(cluster_tickers)} stocks): {', '.join(cluster_tickers)}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "11585057f28da06"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Enhanced Clustering Methods\n",
    "\n",
    "Now, let's try the enhanced clustering methods:"
   ],
   "id": "dc9b38b92a9a801a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Apply advanced clustering methods\n",
    "advanced_methods = ['hierarchical', 'density']\n",
    "advanced_results = {}\n",
    "\n",
    "for method in advanced_methods:\n",
    "    # Set parameters based on method\n",
    "    kwargs = {}\n",
    "    if method == 'hierarchical':\n",
    "        kwargs = {'method': 'ward'}  # Use Ward's method for hierarchical\n",
    "    elif method == 'density':\n",
    "        kwargs = {'min_cluster_size': 3}  # Minimum cluster size for density clustering\n",
    "    \n",
    "    # Cluster the stocks\n",
    "    labels, actual_n_clusters = cluster_stocks(\n",
    "        correlation_matrix, \n",
    "        method=method, \n",
    "        n_clusters=n_clusters,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Get clusters as dictionary\n",
    "    clusters = get_clusters_dict(labels, correlation_matrix.columns.tolist())\n",
    "    \n",
    "    # Store results\n",
    "    advanced_results[method] = {\n",
    "        'labels': labels,\n",
    "        'n_clusters': actual_n_clusters,\n",
    "        'clusters': clusters\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{method.upper()} clustering with {actual_n_clusters} clusters:\")\n",
    "    for cluster_id, cluster_tickers in clusters.items():\n",
    "        print(f\"Cluster {cluster_id} ({len(cluster_tickers)} stocks): {', '.join(cluster_tickers)}\")\n",
    "\n",
    "# Let's try deep embedding clustering on a subset of residual returns (for speed)\n",
    "try:\n",
    "    # Use a subset of data for deep embedding\n",
    "    subset_returns = residual_returns.iloc[-100:]\n",
    "    \n",
    "    deep_labels, embeddings, encoder = deep_embedding_clustering(\n",
    "        subset_returns, \n",
    "        n_clusters=5,  # Fewer clusters for simplicity\n",
    "        embedding_dim=10, \n",
    "        epochs=50\n",
    "    )\n",
    "    \n",
    "    deep_clusters = get_clusters_dict(deep_labels, subset_returns.columns.tolist())\n",
    "    \n",
    "    print(\"\\nDEEP EMBEDDING clustering:\")\n",
    "    for cluster_id, cluster_tickers in deep_clusters.items():\n",
    "        print(f\"Cluster {cluster_id} ({len(cluster_tickers)} stocks): {', '.join(cluster_tickers)}\")\n",
    "        \n",
    "    # Visualize the embeddings\n",
    "    if embeddings is not None and embeddings.shape[1] >= 2:\n",
    "        fig = px.scatter(\n",
    "            x=embeddings[:, 0],\n",
    "            y=embeddings[:, 1],\n",
    "            color=[str(label) for label in deep_labels],\n",
    "            text=subset_returns.columns,\n",
    "            title='Deep Embedding Clustering (2D Projection)',\n",
    "            labels={'x': 'Dimension 1', 'y': 'Dimension 2', 'color': 'Cluster'}\n",
    "        )\n",
    "        fig.update_traces(textposition='top center')\n",
    "        fig.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with deep embedding clustering: {e}\")\n",
    "    print(\"Skipping deep embedding visualization\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "c7e927f04ab83a77"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Clustering Results\n",
    "\n",
    "Visualize the SPONGE clustering result with clusters highlighted:"
   ],
   "id": "b8332f5edf8996f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize clusters for the SPONGE algorithm\n",
    "sponge_labels = clustering_results['sponge_sym']['labels']\n",
    "\n",
    "# Calculate silhouette score to evaluate clustering quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "sil_score = silhouette_score(\n",
    "    1 - np.abs(correlation_matrix.values),  # Convert correlation to distance\n",
    "    sponge_labels,\n",
    "    metric='precomputed'\n",
    ")\n",
    "print(f\"Silhouette Score for SPONGE_SYM clustering: {sil_score:.4f}\")\n",
    "\n",
    "# Get order of stocks sorted by cluster\n",
    "sorted_indices = np.argsort(sponge_labels)\n",
    "sorted_tickers = [correlation_matrix.columns[i] for i in sorted_indices]\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "sorted_corr = correlation_matrix.loc[sorted_tickers, sorted_tickers]\n",
    "\n",
    "# Create a heatmap with cluster boundaries\n",
    "fig = px.imshow(\n",
    "    sorted_corr,\n",
    "    title='Correlation Matrix Sorted by SPONGE_SYM Clusters',\n",
    "    labels=dict(x=\"Stock\", y=\"Stock\", color=\"Correlation\"),\n",
    "    x=sorted_tickers,\n",
    "    y=sorted_tickers,\n",
    "    color_continuous_scale='RdBu_r',\n",
    "    zmin=-1, zmax=1\n",
    ")\n",
    "\n",
    "# Add cluster boundaries\n",
    "cluster_boundaries = []\n",
    "current_cluster = sponge_labels[sorted_indices[0]]\n",
    "\n",
    "for i, idx in enumerate(sorted_indices[1:], 1):\n",
    "    if sponge_labels[idx] != current_cluster:\n",
    "        # Add vertical line\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=i-0.5,\n",
    "            x1=i-0.5,\n",
    "            y0=-0.5,\n",
    "            y1=len(sorted_tickers)-0.5,\n",
    "            line=dict(color=\"black\", width=2)\n",
    "        )\n",
    "        \n",
    "        # Add horizontal line\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=-0.5,\n",
    "            x1=len(sorted_tickers)-0.5,\n",
    "            y0=i-0.5,\n",
    "            y1=i-0.5,\n",
    "            line=dict(color=\"black\", width=2)\n",
    "        )\n",
    "        \n",
    "        current_cluster = sponge_labels[idx]\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "c90cff0722d68abc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adaptive Parameter Management\n",
    "\n",
    "Let's demonstrate how we can use adaptive parameters based on market regimes:"
   ],
   "id": "30446976ddb7a054"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize adaptive parameter manager\n",
    "adaptive_manager = AdaptiveParameterManager(\n",
    "    returns,\n",
    "    lookback_min=3,\n",
    "    lookback_max=20,\n",
    "    rebalance_min=1,\n",
    "    rebalance_max=7,\n",
    "    threshold_min=0.0,\n",
    "    threshold_max=0.01,\n",
    "    correlation_window_min=10,\n",
    "    correlation_window_max=60,\n",
    "    n_regimes=3\n",
    ")\n",
    "\n",
    "# Visualize detected market regimes\n",
    "adaptive_manager.visualize_regimes()\n",
    "\n",
    "# Get optimal parameters for different methods\n",
    "optimal_lookback_vol = adaptive_manager.calculate_optimal_lookback(method='volatility')\n",
    "optimal_lookback_acf = adaptive_manager.calculate_optimal_lookback(method='autocorrelation')\n",
    "optimal_lookback_reg = adaptive_manager.calculate_optimal_lookback(method='regime')\n",
    "\n",
    "optimal_rebalance = adaptive_manager.calculate_optimal_rebalance_period(method='volatility')\n",
    "optimal_threshold = adaptive_manager.calculate_optimal_threshold(method='volatility')\n",
    "optimal_corr_window = adaptive_manager.calculate_optimal_correlation_window(method='volatility')\n",
    "\n",
    "print(\"\\nOptimal Parameters based on current market conditions:\")\n",
    "print(f\"Lookback Window (Volatility): {optimal_lookback_vol} days\")\n",
    "print(f\"Lookback Window (Autocorrelation): {optimal_lookback_acf} days\")\n",
    "print(f\"Lookback Window (Regime): {optimal_lookback_reg} days\")\n",
    "print(f\"Rebalance Period: {optimal_rebalance} days\")\n",
    "print(f\"Threshold: {optimal_threshold:.6f}\")\n",
    "print(f\"Correlation Window: {optimal_corr_window} days\")\n",
    "\n",
    "# Get regime-specific parameters\n",
    "regime_params = adaptive_manager.get_regime_specific_parameters()\n",
    "\n",
    "print(\"\\nRegime-Specific Parameters:\")\n",
    "for key, value in regime_params.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "569172153c9b221d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Portfolio Construction\n",
    "\n",
    "With our clusters defined, we'll now construct mean-reverting portfolios within each cluster."
   ],
   "id": "62d0f4900e2c3e20"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use the SPONGE clustering method for portfolio construction\n",
    "chosen_method = 'sponge_sym'\n",
    "clusters = clustering_results[chosen_method]['clusters']\n",
    "\n",
    "# Set portfolio parameters (or use adaptive parameters)\n",
    "lookback_window = optimal_lookback_vol  # Use adaptive parameter\n",
    "threshold = optimal_threshold  # Use adaptive parameter\n",
    "holding_period = 30  # Days to hold the portfolio\n",
    "\n",
    "# Identify winners and losers\n",
    "winners, losers = identify_winners_losers(\n",
    "    returns.iloc[-lookback_window:],\n",
    "    clusters,\n",
    "    lookback_window,\n",
    "    threshold\n",
    ")\n",
    "\n",
    "# Print winners and losers\n",
    "print(f\"\\nWinners and Losers for {chosen_method.upper()} clusters:\")\n",
    "for cluster_id in clusters.keys():\n",
    "    cluster_winners = winners.get(cluster_id, [])\n",
    "    cluster_losers = losers.get(cluster_id, [])\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Winners ({len(cluster_winners)}): {', '.join(cluster_winners)}\")\n",
    "    print(f\"  Losers ({len(cluster_losers)}): {', '.join(cluster_losers)}\")\n",
    "\n",
    "# Construct arbitrage portfolios\n",
    "portfolios = construct_arbitrage_portfolios(winners, losers)\n",
    "\n",
    "# Print portfolio compositions\n",
    "print(\"\\nPortfolio Compositions:\")\n",
    "for cluster_id, portfolio in portfolios.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for ticker, weight in portfolio.items():\n",
    "        position = \"LONG\" if weight > 0 else \"SHORT\"\n",
    "        print(f\"  {ticker}: {position} {abs(weight):.4f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "af4a597f05aa2e60"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Alternative Data Integration\n",
    "\n",
    "Let's demonstrate how alternative data can be integrated into the portfolio construction process:"
   ],
   "id": "b56387aabe18cbd0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize data processors\n",
    "sentiment_processor = SentimentDataProcessor()\n",
    "options_processor = OptionsDataProcessor()\n",
    "flow_processor = OrderFlowProcessor()\n",
    "\n",
    "# Generate mock data for demonstration\n",
    "print(\"Generating mock alternative data for demonstration...\")\n",
    "\n",
    "# 1. Sentiment data\n",
    "sentiment_data = sentiment_processor.fetch_news_sentiment(\n",
    "    actual_tickers, \n",
    "    returns.index[-60].strftime('%Y-%m-%d'), \n",
    "    returns.index[-1].strftime('%Y-%m-%d'),\n",
    "    provider='mock'\n",
    ")\n",
    "\n",
    "social_sentiment = sentiment_processor.fetch_social_sentiment(\n",
    "    actual_tickers, \n",
    "    returns.index[-60].strftime('%Y-%m-%d'), \n",
    "    returns.index[-1].strftime('%Y-%m-%d'),\n",
    "    provider='mock'\n",
    ")\n",
    "\n",
    "# Aggregate sentiment from different sources\n",
    "aggregated_sentiment = sentiment_processor.aggregate_sentiment(\n",
    "    social_sentiment,\n",
    "    method='exponential_decay',\n",
    "    decay_factor=0.9\n",
    ")\n",
    "\n",
    "print(\"\\nSentiment Data Sample:\")\n",
    "print(aggregated_sentiment.head())\n",
    "\n",
    "# Visualize sentiment over time for a few stocks\n",
    "sample_stocks = actual_tickers[:5]\n",
    "fig = go.Figure()\n",
    "\n",
    "for ticker in sample_stocks:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=aggregated_sentiment.index,\n",
    "        y=aggregated_sentiment[ticker],\n",
    "        mode='lines',\n",
    "        name=ticker\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Sentiment Scores Over Time',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    yaxis_range=[-1, 1],\n",
    "    legend_title='Stock'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 2. Options data\n",
    "options_data = options_processor.fetch_options_data(\n",
    "    actual_tickers[:5],  # Just use a few stocks for speed\n",
    "    returns.index[-60].strftime('%Y-%m-%d'), \n",
    "    returns.index[-1].strftime('%Y-%m-%d'),\n",
    "    provider='mock'\n",
    ")\n",
    "\n",
    "# Extract options signals\n",
    "options_signals = {}\n",
    "for ticker in actual_tickers[:5]:\n",
    "    if ticker in options_data:\n",
    "        options_signals[ticker] = options_processor.extract_options_signals(\n",
    "            options_data, ticker\n",
    "        )\n",
    "\n",
    "print(\"\\nOptions Signals Sample:\")\n",
    "for ticker, signals in list(options_signals.items())[:2]:\n",
    "    print(f\"{ticker}:\")\n",
    "    for signal, value in signals.items():\n",
    "        print(f\"  {signal}: {value:.4f}\")\n",
    "\n",
    "# 3. Order flow data\n",
    "order_flow_data = flow_processor.fetch_order_flow_data(\n",
    "    actual_tickers[:5],\n",
    "    returns.index[-60].strftime('%Y-%m-%d'), \n",
    "    returns.index[-1].strftime('%Y-%m-%d'),\n",
    "    provider='mock'\n",
    ")\n",
    "\n",
    "# Calculate order flow imbalance\n",
    "order_flow_imbalance = {}\n",
    "for ticker, data in order_flow_data.items():\n",
    "    order_flow_imbalance[ticker] = flow_processor.calculate_order_flow_imbalance(\n",
    "        data, resample_freq='1d'\n",
    "    )\n",
    "\n",
    "print(\"\\nOrder Flow Imbalance Sample:\")\n",
    "for ticker, imbalance in list(order_flow_imbalance.items())[:2]:\n",
    "    print(f\"{ticker}:\")\n",
    "    print(imbalance.head())\n",
    "\n",
    "# Apply alternative data to portfolio construction\n",
    "# 1. Adjust returns using sentiment\n",
    "sentiment_adjusted_returns = returns.iloc[-lookback_window:].copy()\n",
    "for date in sentiment_adjusted_returns.index:\n",
    "    if date in aggregated_sentiment.index:\n",
    "        for ticker in sentiment_adjusted_returns.columns:\n",
    "            if ticker in aggregated_sentiment.columns:\n",
    "                sentiment_score = aggregated_sentiment.loc[date, ticker]\n",
    "                sentiment_adjusted_returns.loc[date, ticker] *= (1 + sentiment_score * 0.1)\n",
    "\n",
    "# 2. Identify winners and losers with sentiment-adjusted returns\n",
    "sentiment_winners, sentiment_losers = identify_winners_losers(\n",
    "    sentiment_adjusted_returns,\n",
    "    clusters,\n",
    "    lookback_window,\n",
    "    threshold\n",
    ")\n",
    "\n",
    "# 3. Apply options signals\n",
    "for ticker, signals in options_signals.items():\n",
    "    for cluster_id in winners.keys():\n",
    "        # If high put/call ratio, move from winners to losers\n",
    "        if ticker in sentiment_winners.get(cluster_id, []) and signals.get('put_call_ratio', 1.0) > 1.3:\n",
    "            sentiment_winners[cluster_id].remove(ticker)\n",
    "            if ticker not in sentiment_losers[cluster_id]:\n",
    "                sentiment_losers[cluster_id].append(ticker)\n",
    "            print(f\"Moved {ticker} from winners to losers due to high put/call ratio\")\n",
    "            \n",
    "        # If very negative skew, move from losers to winners\n",
    "        elif ticker in sentiment_losers.get(cluster_id, []) and signals.get('skew_spread', 0) < -10:\n",
    "            sentiment_losers[cluster_id].remove(ticker)\n",
    "            if ticker not in sentiment_winners[cluster_id]:\n",
    "                sentiment_winners[cluster_id].append(ticker)\n",
    "            print(f\"Moved {ticker} from losers to winners due to negative skew\")\n",
    "\n",
    "# Construct enhanced portfolios\n",
    "enhanced_portfolios = construct_arbitrage_portfolios(sentiment_winners, sentiment_losers)\n",
    "\n",
    "print(\"\\nEnhanced Portfolio Compositions (with Alternative Data):\")\n",
    "for cluster_id, portfolio in enhanced_portfolios.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for ticker, weight in portfolio.items():\n",
    "        position = \"LONG\" if weight > 0 else \"SHORT\"\n",
    "        print(f\"  {ticker}: {position} {abs(weight):.4f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "924b28d377e7b774"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Portfolio Evaluation\n",
    "\n",
    "We'll evaluate the performance of both standard and enhanced portfolios."
   ],
   "id": "a9f9b096fb760c13"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define evaluation period (last 30 days of data)\n",
    "evaluation_period = 30\n",
    "evaluation_returns = returns.iloc[-evaluation_period:]\n",
    "\n",
    "# Calculate standard portfolio returns\n",
    "standard_portfolio_returns = calculate_portfolio_returns(\n",
    "    evaluation_returns, portfolios\n",
    ")\n",
    "\n",
    "# Calculate enhanced portfolio returns\n",
    "enhanced_portfolio_returns = calculate_portfolio_returns(\n",
    "    evaluation_returns, enhanced_portfolios\n",
    ")\n",
    "\n",
    "# Display portfolio returns statistics\n",
    "print(\"\\nStandard Portfolio Returns Statistics:\")\n",
    "print(f\"Mean Daily Return: {standard_portfolio_returns['Combined'].mean():.4%}\")\n",
    "print(f\"Standard Deviation: {standard_portfolio_returns['Combined'].std():.4%}\")\n",
    "print(f\"Sharpe Ratio: {calculate_sharpe_ratio(standard_portfolio_returns['Combined']):.4f}\")\n",
    "print(f\"Sortino Ratio: {calculate_sortino_ratio(standard_portfolio_returns['Combined']):.4f}\")\n",
    "print(f\"Cumulative Return: {(1 + standard_portfolio_returns['Combined']).prod() - 1:.4%}\")\n",
    "print(f\"Annualized Return: {calculate_annualized_return(standard_portfolio_returns['Combined']):.4%}\")\n",
    "print(f\"Win Rate: {(standard_portfolio_returns['Combined'] > 0).mean():.4%}\")\n",
    "\n",
    "print(\"\\nEnhanced Portfolio Returns Statistics:\")\n",
    "print(f\"Mean Daily Return: {enhanced_portfolio_returns['Combined'].mean():.4%}\")\n",
    "print(f\"Standard Deviation: {enhanced_portfolio_returns['Combined'].std():.4%}\")\n",
    "print(f\"Sharpe Ratio: {calculate_sharpe_ratio(enhanced_portfolio_returns['Combined']):.4f}\")\n",
    "print(f\"Sortino Ratio: {calculate_sortino_ratio(enhanced_portfolio_returns['Combined']):.4f}\")\n",
    "print(f\"Cumulative Return: {(1 + enhanced_portfolio_returns['Combined']).prod() - 1:.4%}\")\n",
    "print(f\"Annualized Return: {calculate_annualized_return(enhanced_portfolio_returns['Combined']):.4%}\")\n",
    "print(f\"Win Rate: {(enhanced_portfolio_returns['Combined'] > 0).mean():.4%}\")\n",
    "\n",
    "# Calculate cumulative returns\n",
    "std_cumulative = (1 + standard_portfolio_returns['Combined']).cumprod() - 1\n",
    "enhanced_cumulative = (1 + enhanced_portfolio_returns['Combined']).cumprod() - 1\n",
    "\n",
    "# Plot cumulative returns comparison\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=std_cumulative.index,\n",
    "    y=std_cumulative,\n",
    "    mode='lines',\n",
    "    name='Standard Portfolio'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=enhanced_cumulative.index,\n",
    "    y=enhanced_cumulative,\n",
    "    mode='lines',\n",
    "    name='Enhanced Portfolio'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Cumulative Portfolio Returns',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Return',\n",
    "    yaxis_tickformat='.1%',\n",
    "    legend_title='Portfolio Type'\n",
    ")\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "ff98232eb09d138c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Backtesting\n",
    "\n",
    "Let's demonstrate the advanced backtesting features:"
   ],
   "id": "2571e49545e0a683"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize a backtest instance for the full period\n",
    "backtest = StatisticalArbitrageBacktest(\n",
    "    tickers=actual_tickers,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    clustering_method='sponge_sym',\n",
    "    beta_window=60,\n",
    "    correlation_window=20,\n",
    "    lookback_window=5,\n",
    "    rebalance_period=3,\n",
    "    threshold=0.0,\n",
    "    stop_win_threshold=0.05\n",
    ")\n",
    "\n",
    "# Run the backtest\n",
    "print(\"Running full backtest...\")\n",
    "portfolio_returns = backtest.run_backtest()\n",
    "metrics = backtest.get_performance_metrics()\n",
    "\n",
    "print(\"\\nFull Backtest Metrics:\")\n",
    "print(metrics.loc['Combined'])\n",
    "\n",
    "# Get market benchmark returns (SPY)\n",
    "spy_returns = returns['SPY']\n",
    "\n",
    "# Plot cumulative returns comparison with benchmark\n",
    "plot_cumulative_returns(\n",
    "    {'SPONGE Clustering': portfolio_returns},\n",
    "    spy_returns,\n",
    "    title='Cumulative Returns: Strategy vs Market'\n",
    ")\n",
    "\n",
    "# Plot drawdowns\n",
    "plot_drawdowns(\n",
    "    {'SPONGE Clustering': portfolio_returns},\n",
    "    title='Strategy Drawdowns'\n",
    ")\n",
    "\n",
    "# Plot monthly returns heatmap\n",
    "plot_monthly_returns_heatmap(\n",
    "    {'SPONGE Clustering': portfolio_returns},\n",
    "    'SPONGE Clustering',\n",
    "    title='Monthly Returns: SPONGE Clustering'\n",
    ")\n",
    "\n",
    "# Time-Series Cross-Validation (simplified for speed)\n",
    "print(\"\\nRunning Time-Series Cross-Validation (simplified)...\")\n",
    "# Note: This would typically take longer, so we're using a smaller subset and fewer splits\n",
    "tscv_results = time_series_cross_validation(\n",
    "    actual_tickers[:15],  # Use fewer tickers for speed\n",
    "    start_date,\n",
    "    end_date,\n",
    "    n_splits=2,  # Fewer splits for example\n",
    "    backtest_params={\n",
    "        'clustering_method': 'spectral',  # Faster method\n",
    "        'lookback_window': 5,\n",
    "        'rebalance_period': 3,\n",
    "        'threshold': 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nTime-Series CV Results:\")\n",
    "if 'mean_sharpe' in tscv_results:\n",
    "    print(f\"Mean Sharpe: {tscv_results['mean_sharpe']:.4f} ± {tscv_results['std_sharpe']:.4f}\")\n",
    "    print(f\"Mean Sortino: {tscv_results['mean_sortino']:.4f} ± {tscv_results['std_sortino']:.4f}\")\n",
    "    print(f\"Mean Annualized Return: {tscv_results['mean_annualized_return']:.4%} ± {tscv_results['std_annualized_return']:.4%}\")\n",
    "else:\n",
    "    print(\"Cross-validation did not complete successfully. Check logs for details.\")\n",
    "\n",
    "# Monte Carlo Simulation (simplified for speed)\n",
    "print(\"\\nRunning Monte Carlo Simulation (simplified)...\")\n",
    "mc_results = monte_carlo_backtest(\n",
    "    returns.iloc[-252:],  # Use only last year for speed\n",
    "    residual_returns.iloc[-252:],  # Use only last year for speed\n",
    "    n_simulations=50,  # Fewer simulations for example\n",
    "    backtest_params={\n",
    "        'clustering_method': 'spectral',  # Faster method\n",
    "        'lookback_window': 5,\n",
    "        'rebalance_period': 3,\n",
    "        'threshold': 0.0\n",
    "    },\n",
    "    tickers=actual_tickers[:15]  # Use fewer tickers for speed\n",
    ")\n",
    "\n",
    "print(\"\\nMonte Carlo Results:\")\n",
    "if 'mean' in mc_results:\n",
    "    print(f\"Mean Sharpe: {mc_results['mean']['sharpe_ratio']:.4f} ± {mc_results['std']['sharpe_ratio']:.4f}\")\n",
    "    print(f\"5th-95th Percentile Sharpe: [{mc_results['5th_percentile']['sharpe_ratio']:.4f}, {mc_results['95th_percentile']['sharpe_ratio']:.4f}]\")\n",
    "    print(f\"Mean Annualized Return: {mc_results['mean']['annualized_return']:.4%} ± {mc_results['std']['annualized_return']:.4%}\")\n",
    "else:\n",
    "    print(\"Monte Carlo simulation did not complete successfully. Check logs for details.\")\n",
    "\n",
    "# Walk-Forward Optimization (simplified for speed)\n",
    "print(\"\\nRunning Walk-Forward Optimization (simplified)...\")\n",
    "wf_results = walk_forward_optimization(\n",
    "    actual_tickers[:15],  # Use fewer tickers for speed\n",
    "    start_date,\n",
    "    end_date,\n",
    "    initial_window=252,\n",
    "    step=63,  # Quarterly steps\n",
    "    param_grid={\n",
    "        'lookback_window': [3, 10],  # Simplified grid\n",
    "        'rebalance_period': [1, 5],\n",
    "        'threshold': [0.0, 0.005]\n",
    "    },\n",
    "    clustering_method='spectral'  # Faster method\n",
    ")\n",
    "\n",
    "print(\"\\nWalk-Forward Optimization Results:\")\n",
    "if 'weighted_sharpe' in wf_results:\n",
    "    print(f\"Weighted Sharpe: {wf_results['weighted_sharpe']:.4f}\")\n",
    "    print(f\"Weighted Annualized Return: {wf_results['weighted_annual_return']:.4%}\")\n",
    "\n",
    "    print(\"\\nParameter Frequency:\")\n",
    "    for param, freq in wf_results['parameter_frequency'].items():\n",
    "        print(f\"{param}:\", end=\" \")\n",
    "        sorted_freq = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        for value, count in sorted_freq:\n",
    "            print(f\"{value}: {count}\", end=\", \")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Walk-forward optimization did not complete successfully. Check logs for details.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "2e8effa7c3cbc28b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison to Industry Benchmark\n",
    "\n",
    "Let's compare our clustering approach to a traditional industry-based approach:"
   ],
   "id": "b8214887cd5875ee"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get Fama-French 12 industry classifications\n",
    "industry_mapping = get_fama_french_industries(actual_tickers)\n",
    "industry_names = get_industry_names()\n",
    "\n",
    "# Print industry classifications\n",
    "print(\"\\nFama-French 12 Industry Classifications:\")\n",
    "for ticker, industry_id in sorted(industry_mapping.items()):\n",
    "    industry_name = industry_names.get(industry_id, \"Unknown\")\n",
    "    print(f\"{ticker}: {industry_id} - {industry_name}\")\n",
    "\n",
    "# Run industry benchmark\n",
    "print(\"\\nRunning industry-based benchmark...\")\n",
    "industry_returns, industry_metrics = run_industry_benchmark(\n",
    "    tickers=actual_tickers,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    industry_mapping=industry_mapping,\n",
    "    beta_window=60,\n",
    "    correlation_window=20,\n",
    "    lookback_window=5,\n",
    "    rebalance_period=3,\n",
    "    threshold=0.0,\n",
    "    stop_win_threshold=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nIndustry Benchmark Metrics:\")\n",
    "print(industry_metrics.loc['Combined'])\n",
    "\n",
    "# Compare cumulative returns\n",
    "clustering_cumulative = (1 + portfolio_returns['Combined']).cumprod() - 1\n",
    "industry_cumulative = (1 + industry_returns['Combined']).cumprod() - 1\n",
    "spy_cumulative = (1 + spy_returns[clustering_cumulative.index[0]:clustering_cumulative.index[-1]]).cumprod() - 1\n",
    "\n",
    "# Plot comparison\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=clustering_cumulative.index,\n",
    "    y=clustering_cumulative,\n",
    "    mode='lines',\n",
    "    name='Clustering Approach'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=industry_cumulative.index,\n",
    "    y=industry_cumulative,\n",
    "    mode='lines',\n",
    "    name='Industry Benchmark'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=spy_cumulative.index,\n",
    "    y=spy_cumulative,\n",
    "    mode='lines',\n",
    "    name='S&P 500 (SPY)',\n",
    "    line=dict(dash='dash')\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Cumulative Returns Comparison',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Return',\n",
    "    yaxis_tickformat='.1%',\n",
    "    legend_title='Strategy'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Compare key metrics\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Clustering': metrics.loc['Combined', ['Annualized Return (%)', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown (%)']],\n",
    "    'Industry': industry_metrics.loc['Combined', ['Annualized Return (%)', 'Sharpe Ratio', 'Sortino Ratio', 'Maximum Drawdown (%)']]\n",
    "})\n",
    "\n",
    "print(\"\\nMetrics Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Clustering', x=comparison_df.index, y=comparison_df['Clustering']),\n",
    "    go.Bar(name='Industry', x=comparison_df.index, y=comparison_df['Industry'])\n",
    "])\n",
    "fig.update_layout(\n",
    "    barmode='group',\n",
    "    title='Performance Metrics Comparison',\n",
    "    xaxis_title='Metric',\n",
    "    yaxis_title='Value'\n",
    ")\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "e78d4fa4e17d9f9d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Portfolio Integration\n",
    "\n",
    "Finally, let's demonstrate how to integrate our statistical arbitrage strategy with existing portfolios:"
   ],
   "id": "42b8512893a9e3d4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create mock strategy returns for portfolio integration\n",
    "# S&P 500\n",
    "spy_strategy = spy_returns\n",
    "\n",
    "# Value strategy (higher returns during value regimes)\n",
    "dates = returns.index\n",
    "value_returns = pd.Series(index=dates, dtype=float)\n",
    "for i in range(len(dates)):\n",
    "    # Add some mean-reversion and value-growth cycle\n",
    "    cycle = np.sin(i / 180 * np.pi)  # Roughly 1.5 year value-growth cycle\n",
    "    # More positive during value regimes, negative during growth regimes\n",
    "    value_returns[i] = 0.0003 + 0.001 * cycle + np.random.normal(0, 0.008)\n",
    "\n",
    "# Momentum strategy\n",
    "momentum_returns = pd.Series(index=dates, dtype=float)\n",
    "for i in range(len(dates)):\n",
    "    # Add trend persistence\n",
    "    if i > 0:\n",
    "        momentum_returns[i] = 0.0004 + 0.2 * momentum_returns[i-1] + np.random.normal(0, 0.009)\n",
    "    else:\n",
    "        momentum_returns[i] = 0.0004 + np.random.normal(0, 0.009)\n",
    "\n",
    "# Use our statistical arbitrage strategy\n",
    "stat_arb_returns = portfolio_returns['Combined']\n",
    "\n",
    "# Create dictionary of strategies\n",
    "strategies = {\n",
    "    'S&P 500': spy_strategy,\n",
    "    'Value': value_returns,\n",
    "    'Momentum': momentum_returns,\n",
    "    'StatArb': stat_arb_returns\n",
    "}\n",
    "\n",
    "# Make sure all strategies have the same date range\n",
    "common_dates = stat_arb_returns.index\n",
    "for name in strategies:\n",
    "    strategies[name] = strategies[name].loc[strategies[name].index.isin(common_dates)]\n",
    "\n",
    "# Initialize portfolio allocator\n",
    "allocator = PortfolioAllocator(strategies, risk_free_rate=0.005)\n",
    "\n",
    "# Print strategy metrics\n",
    "print(\"\\nStrategy Metrics:\")\n",
    "print(allocator.metrics)\n",
    "\n",
    "# Print correlation matrix\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(allocator.correlation_matrix)\n",
    "\n",
    "# Try different allocation methods\n",
    "for method in ['risk_parity', 'min_variance', 'max_diversification', 'equal_weight']:\n",
    "    weights = allocator.optimize_allocation(method)\n",
    "    print(f\"\\n{method.replace('_', ' ').title()} Weights:\")\n",
    "    print(weights)\n",
    "    \n",
    "    metrics = allocator.calculate_portfolio_metrics(weights)\n",
    "    print(f\"\\n{method.replace('_', ' ').title()} Portfolio Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Cluster strategies to find natural groupings\n",
    "clustering = allocator.cluster_strategies()\n",
    "print(\"\\nStrategy Clusters:\")\n",
    "for cluster_id, strategies in clustering['clusters'].items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(strategies)}\")\n",
    "\n",
    "# Calculate impact of adding statistical arbitrage to a portfolio of SPY\n",
    "impact = calculate_information_ratio_impact(\n",
    "    spy_strategy,\n",
    "    stat_arb_returns,\n",
    "    allocation=0.15\n",
    ")\n",
    "\n",
    "print(\"\\nImpact of Adding Statistical Arbitrage to S&P 500:\")\n",
    "for key, value in impact.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Visualize the efficient frontier\n",
    "allocator.plot_efficient_frontier(n_portfolios=1000)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "54aa8cf384b16bd5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've implemented the correlation matrix clustering approach for statistical arbitrage from Cartea, Cucuringu, and Jin (2023), along with significant enhancements:\n",
    "\n",
    "1. **Advanced Factor Models**: Beyond CAPM, we've explored Fama-French and PCA-based factor models to better isolate stock-specific returns.\n",
    "\n",
    "2. **Enhanced Clustering Methods**: In addition to the original methods, we've implemented hierarchical, density-based, and deep embedding clustering approaches.\n",
    "\n",
    "3. **Adaptive Parameter Management**: We've demonstrated how to dynamically adjust strategy parameters based on market regimes.\n",
    "\n",
    "4. **Alternative Data Integration**: We've shown how sentiment, options, and order flow data can enhance clustering and signal generation.\n",
    "\n",
    "5. **Advanced Backtesting**: We've demonstrated time-series cross-validation, Monte Carlo simulation, and walk-forward optimization for robust performance evaluation.\n",
    "\n",
    "6. **Portfolio Integration**: We've shown how to optimally combine the statistical arbitrage strategy with existing portfolios.\n",
    "\n",
    "The results show that clustering-based statistical arbitrage can identify temporary mispricings within groups of similar stocks, and our enhancements can further improve strategy performance and robustness."
   ],
   "id": "7279859aabcea240"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
